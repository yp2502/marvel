# -*- coding: utf-8 -*-
"""E03_MLL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R2-k-411GIuHdByo5OgxdPoKAP6YxSBC
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

csv_path = 'adult_dataset.csv'
df = pd.read_csv(csv_path)

print(df.head())

print ("Rows     : \n" ,df.shape[0])
print ("Columns  : \n" ,df.shape[1])
print ("\nFeatures : \n" ,df.columns.tolist())
print ("\nMissing values : \n", df.isnull().sum().values.sum())
print ("\nUnique values : \n", df.nunique())

df.info()

print(df.describe())

df_missing_workclass = (df['workclass']=='?').sum()
df_missing_workclass

df_missing = (df=='?').sum()
df_missing

percent_missing = (df=='?').sum() * 100/len(df)
percent_missing

df.apply(lambda x: x !='?',axis=1).sum()

df_categorical = df.select_dtypes(include=['object'])

# checking whether any other column contains '?' value
df_categorical.apply(lambda x: x=='?',axis=1).sum()

df = df[df['native.country'] != '?']
df = df[df['occupation'] !='?']

print(df)

df.info()

from sklearn import preprocessing

# encode categorical variables using label Encoder
# select all categorical variables
df_categorical = df.select_dtypes(include=['object'])
print(df_categorical.head())

#appy label encoding
le = preprocessing.LabelEncoder()
df_categorical = df_categorical.apply(le.fit_transform)
print(df_categorical.head())

df = df.drop(df_categorical.columns,axis=1)
print(df)

df = pd.concat([df,df_categorical],axis=1)
print(df.head())

df['income'] = df['income'].astype('category')

print(df)

from sklearn.model_selection import train_test_split

# independent features to X
X = df.drop('income',axis=1)

# dependent variable to Y
Y = df['income']

print(X.head())

Y.head()

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.30,random_state=99)

print(X_train.head())

Y_train.head()

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("Y_train shape:", Y_train.shape)
print("Y_test shape:", Y_test.shape)

from sklearn.tree import DecisionTreeClassifier
dec_tree = DecisionTreeClassifier(max_depth=5, random_state=42)

dec_tree.fit(X_train, Y_train)

Y_pred_dec_tree = dec_tree.predict(X_test)
Y_pred_dec_tree

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

print('Decision Tree Classifier:')
print('Accuracy score:', round(accuracy_score(Y_test, Y_pred_dec_tree) * 100, 2))
print('F1 score:', round(f1_score(Y_test, Y_pred_dec_tree) * 100, 2))

from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
cm = confusion_matrix(Y_test, Y_pred_dec_tree)
cm

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Reds')

from sklearn import tree
import matplotlib.pyplot as plt

# Assuming 'clf' is your trained decision tree classifier
plt.figure(figsize=(20,10))
tree.plot_tree(dec_tree, filled=True)
plt.show()

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search
param_grid = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy'],
    'max_features': [None, 'sqrt', 'log2']
}

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                           param_grid=param_grid,
                           scoring='accuracy',  # You can change this to 'f1' if you prefer
                           cv=5,  # 5-fold cross-validation
                           verbose=1,
                           n_jobs=-1)

# Fit the model using GridSearchCV
grid_search.fit(X_train, Y_train)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_}")

best_dec_tree = grid_search.best_estimator_
Y_pred_best_dec_tree = best_dec_tree.predict(X_test)

print('Tuned Decision Tree Classifier:')
print('Accuracy score:', round(accuracy_score(Y_test, Y_pred_best_dec_tree) * 100, 2))
print('F1 score:', round(f1_score(Y_test, Y_pred_best_dec_tree) * 100, 2))

cm_best = confusion_matrix(Y_test, Y_pred_best_dec_tree)
disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best)
disp_best.plot(cmap='Blues')

plt.figure(figsize=(20,10))
tree.plot_tree(best_dec_tree, max_depth=5, filled=True, fontsize=10)
plt.title('Optimized Decision Tree (Depth = 5)')
plt.show()

"""Before Hyperparameter Tuning

> Add blockquote


"""

from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix

precision_before = precision_score(Y_test, Y_pred_dec_tree)
recall_before = recall_score(Y_test, Y_pred_dec_tree)
accuracy_before = accuracy_score(Y_test, Y_pred_dec_tree)
f1_before = f1_score(Y_test, Y_pred_dec_tree)
confusion_matrix_before = confusion_matrix(Y_test, Y_pred_dec_tree)

print("Before Tuning")
print(f"Accuracy: {accuracy_before:.2f}")
print(f"F1 Score: {f1_before:.2f}")
print(f"Precision: {precision_before:.2f}")
print(f"Recall: {recall_before:.2f}")
print(f"Confusion Matrix: \n{confusion_matrix_before}")

"""After Hyperparameter Tuning


"""

precision_after = precision_score(Y_test, Y_pred_best_dec_tree)
recall_after = recall_score(Y_test, Y_pred_best_dec_tree)
accuracy_after = accuracy_score(Y_test, Y_pred_best_dec_tree)
f1_after = f1_score(Y_test, Y_pred_best_dec_tree)
confusion_matrix_after = confusion_matrix(Y_test, Y_pred_best_dec_tree)

print("After Tuning")
print(f"Accuracy: {accuracy_after:.2f}")
print(f"F1 Score: {f1_after:.2f}")
print(f"Precision: {precision_after:.2f}")
print(f"Recall: {recall_after:.2f}")
print(f"Confusion Matrix: \n{confusion_matrix_after}")