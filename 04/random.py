# -*- coding: utf-8 -*-
"""E04_MLL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WhrtfBV2nneDbFvFqupY_rIaP8J7ydLJ
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

csv_path = 'adult_dataset.csv'
df = pd.read_csv(csv_path)

print(df.head())

print ("Rows     : \n" ,df.shape[0])
print ("Columns  : \n" ,df.shape[1])
print ("\nFeatures : \n" ,df.columns.tolist())
print ("\nMissing values : \n", df.isnull().sum().values.sum())
print ("\nUnique values : \n", df.nunique())

df.info()

print(df.describe())

df_missing_workclass = (df['workclass']=='?').sum()
df_missing_workclass

df_missing = (df=='?').sum()
df_missing

percent_missing = (df=='?').sum() * 100/len(df)
percent_missing

df.apply(lambda x: x !='?',axis=1).sum()

df_categorical = df.select_dtypes(include=['object'])

# checking whether any other column contains '?' value
df_categorical.apply(lambda x: x=='?',axis=1).sum()

df = df[df['native.country'] != '?']
df = df[df['occupation'] !='?']

print(df)

df.info()

from sklearn import preprocessing

# encode categorical variables using label Encoder
# select all categorical variables
df_categorical = df.select_dtypes(include=['object'])
print(df_categorical.head())

#appy label encoding
le = preprocessing.LabelEncoder()
df_categorical = df_categorical.apply(le.fit_transform)
print(df_categorical.head())

df = df.drop(df_categorical.columns,axis=1)
print(df)

df = pd.concat([df,df_categorical],axis=1)
print(df.head())

df['income'] = df['income'].astype('category')

print(df)

from sklearn.model_selection import train_test_split

# independent features to X
X = df.drop('income',axis=1)

# dependent variable to Y
Y = df['income']

print(X.head())

Y.head()

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.30,random_state=99)

print(X_train.head())

Y_train.head()

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("Y_train shape:", Y_train.shape)
print("Y_test shape:", Y_test.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay

# Initialize the Random Forest model
rf = RandomForestClassifier(random_state=42)

# Fit the model on the training data
rf.fit(X_train, Y_train)

# Predict the labels on the test data
Y_pred_rf = rf.predict(X_test)

# Evaluate the performance of the model
print('Random Forest Classifier:')
print('Accuracy score:', round(accuracy_score(Y_test, Y_pred_rf) * 100, 2))
print('F1 score:', round(f1_score(Y_test, Y_pred_rf) * 100, 2))

# Confusion matrix
cm_rf = confusion_matrix(Y_test, Y_pred_rf)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf)
disp_rf.plot(cmap='Reds')

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid to search
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy'],
    'max_features': [None, 'sqrt', 'log2']
}

# Create the RandomizedSearchCV object
random_search_rf = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42),
                                      param_distributions=param_grid_rf,
                                      n_iter=20,  # Number of parameter settings that are sampled
                                      scoring='accuracy',
                                      cv=3,  # 3-fold cross-validation
                                      verbose=1,
                                      n_jobs=-1,
                                      random_state=42)

# Fit the model using RandomizedSearchCV
random_search_rf.fit(X_train, Y_train)

# Best parameters and score
print(f"Best Parameters: {random_search_rf.best_params_}")
print(f"Best Score: {random_search_rf.best_score_}")

# Use the best estimator to predict the test set
best_rf = random_search_rf.best_estimator_
Y_pred_best_rf = best_rf.predict(X_test)

print('Tuned Random Forest Classifier:')
print('Accuracy score:', round(accuracy_score(Y_test, Y_pred_best_rf) * 100, 2))
print('F1 score:', round(f1_score(Y_test, Y_pred_best_rf) * 100, 2))

# Confusion matrix for the tuned model
cm_best_rf = confusion_matrix(Y_test, Y_pred_best_rf)
disp_best_rf = ConfusionMatrixDisplay(confusion_matrix=cm_best_rf)
disp_best_rf.plot(cmap='Blues')

from sklearn import tree

# Plot one of the trees in the Random Forest (for visualization)
plt.figure(figsize=(20, 10))
tree.plot_tree(best_rf.estimators_[0], max_depth=5, filled=True, fontsize=10)
plt.title('Optimized Random Forest Tree (Depth = 5)')
plt.show()

from sklearn.metrics import precision_score, recall_score

# Before tuning
precision_before = precision_score(Y_test, Y_pred_rf)
recall_before = recall_score(Y_test, Y_pred_rf)
accuracy_before = accuracy_score(Y_test, Y_pred_rf)
f1_before = f1_score(Y_test, Y_pred_rf)
confusion_matrix_before = confusion_matrix(Y_test, Y_pred_rf)

print("Before Tuning")
print(f"Accuracy: {accuracy_before:.2f}")
print(f"F1 Score: {f1_before:.2f}")
print(f"Precision: {precision_before:.2f}")
print(f"Recall: {recall_before:.2f}")
print(f"Confusion Matrix: \n{confusion_matrix_before}")

# After tuning
precision_after = precision_score(Y_test, Y_pred_best_rf)
recall_after = recall_score(Y_test, Y_pred_best_rf)
accuracy_after = accuracy_score(Y_test, Y_pred_best_rf)
f1_after = f1_score(Y_test, Y_pred_best_rf)
confusion_matrix_after = confusion_matrix(Y_test, Y_pred_best_rf)

print("After Tuning")
print(f"Accuracy: {accuracy_after:.2f}")
print(f"F1 Score: {f1_after:.2f}")
print(f"Precision: {precision_after:.2f}")
print(f"Recall: {recall_after:.2f}")
print(f"Confusion Matrix: \n{confusion_matrix_after}")